#!/usr/bin/env python3
#
# Generates gitlab child pipelines for building images.
# Each pipeline should build a single image configuration test.
import hashlib
import json
import os
import pathlib
import subprocess as sp
import sys

TEST_CACHE_ROOT = os.path.expanduser("~/.cache/osbuild-images")

CONFIGS_PATH = "./test/configs"
CONFIG_MAP = "./test/config-map.json"

S3_BUCKET = "s3://image-builder-ci-artifacts"
S3_PREFIX = "images/builds"

# skip image types that we can't test right now
SKIPS = [
    "edge-ami",
    "edge-vsphere",
    "edge-installer",
    "edge-raw-image",
    "edge-simplified-installer",
    "iot-installer",
    "iot-raw-image",
    "iot-simplified-installer",
]

# ostree containers are pushed to the CI registry to be reused by dependants
OSTREE_CONTAINERS = [
    "iot-container",
    "edge-container"
]

# base and terraform bits copied from main .gitlab-ci.yml
# needed for status reporting and defining the runners
BASE_CONFIG = """
.base:
  before_script:
    - mkdir -p /tmp/artifacts
    - schutzbot/ci_details.sh > /tmp/artifacts/ci-details-before-run.txt
    - cat schutzbot/team_ssh_keys.txt | tee -a ~/.ssh/authorized_keys > /dev/null
  after_script:
    - schutzbot/ci_details.sh > /tmp/artifacts/ci-details-after-run.txt || true
    - schutzbot/update_github_status.sh update || true
    - schutzbot/save_journal.sh || true
    - schutzbot/upload_artifacts.sh
  interruptible: true
  retry: 1
  tags:
    - terraform

.terraform:
  extends: .base
  tags:
    - terraform
"""

JOB_TEMPLATE = """
build/{distro}/{arch}/{image_type}/{config_name}:
  stage: test
  script: |
    ./test/cases/build-image.sh "{distro}" "{image_type}" "{config}"
    {extra_commands}
  extends: .terraform
  variables:
    RUNNER: aws/fedora-38-{arch}
    INTERNAL_NETWORK: "{internal}"
"""

NULL_CONFIG = """
NullBuild:
  stage: test
  script: "true"
"""


def runcmd(cmd, stdin=None):
    job = sp.run(cmd, input=stdin, capture_output=True)
    if job.returncode > 0:
        print(f"Command failed: {cmd}")
        if job.stdout:
            print(job.stdout.decode())
        if job.stderr:
            print(job.stderr.decode())
        sys.exit(job.returncode)

    return job.stdout, job.stderr


def check_config_names():
    """
    Check that all the configs we rely on have names that match the file name, otherwise the test skipping and pipeline
    generation will be incorrect.
    """
    bad_configs = []
    for file in pathlib.Path(CONFIGS_PATH).glob("*.json"):
        config = json.loads(file.read_text())
        if file.stem != config["name"]:
            bad_configs.append(str(file))

    if bad_configs:
        print("ERROR: The following test configs have names that don't match their filenames.")
        print("\n".join(bad_configs))
        print("This will produce incorrect test generation and results.")
        print("Aborting.")
        sys.exit(1)


def generate_manifests(outputdir, arch_arg):
    cmd = ["go", "run", "./cmd/gen-manifests",
           "-cache", os.path.join(TEST_CACHE_ROOT, "rpmmd"),
           "-output", outputdir,
           "-workers", "100",
           "-arches", arch_arg]
    print(f"Running: {' '.join(cmd)}", flush=True)
    out, err = runcmd(cmd)

    # print stderr in case there were errors or warnings about skipped configurations
    # but filter out the annoying ones
    stderr = err.decode().splitlines()
    for line in stderr:
        if "No match for group package" in line:
            continue
        if "Failed to load consumer certs" in line:
            continue
        print(line)

    print("Manifest generation done!\n")


def s3_auth_args():
    s3_key = os.environ.get("V2_AWS_SECRET_ACCESS_KEY")
    s3_key_id = os.environ.get("V2_AWS_ACCESS_KEY_ID")
    if s3_key and s3_key_id:
        return [f"--access_key={s3_key_id}", f"--secret_key={s3_key}"]

    return []


def dl_s3_configs(destination):
    """
    Downloads all the configs from the s3 bucket.
    """
    s3url = f"{S3_BUCKET}/{S3_PREFIX}"
    print(f"Downloading configs from {s3url}")
    job = sp.run(["s3cmd", *s3_auth_args(), "sync", s3url, destination], capture_output=True)
    ok = job.returncode == 0
    if not ok:
        print(f"Failed to sync contents of {s3url}:")
        print(job.stderr.decode())
    return ok


def serialise(data):
    """
    Serialises a manifest to match the way we save manifests in ./cmd/build.
    - Indents with 2 spaces.
    - Adds newline at the end of the file.
    - Escapes < and > in a string to match the behaviour of Go's MarshalJSON.
    """
    serialised = json.dumps(data, indent="  ") + "\n"
    return serialised.replace("<", r"\u003c").replace(">", r"\u003e")


def get_manifest_id(manifest_data):
    md = json.dumps(manifest_data).encode()
    out, _ = runcmd(["osbuild", "--inspect", "-"], stdin=md)
    data = json.loads(out)
    # last stage ID depends on all previous stage IDs, so we can use it as a manifest ID
    return data["pipelines"][-1]["stages"][-1]["id"]


def resync_s3_configs(source):
    s3url = f"{S3_BUCKET}/{S3_PREFIX}/"
    print(f"Uploading configs to {s3url}")
    job = sp.run(["s3cmd", *s3_auth_args(), "sync", "--no-delete-removed", "--skip-existing", source, s3url],
                 capture_output=True)
    ok = job.returncode == 0
    if not ok:
        print(f"Failed to sync contents of {s3url}:")
        print(job.stderr.decode())
    return ok


def filter_builds(manifest_dir):
    print("Filtering build configurations")
    dl_path = os.path.join(TEST_CACHE_ROOT, "s3configs")
    os.makedirs(dl_path, exist_ok=True)
    build_requests = []

    dl_s3_configs(dl_path)

    errors = []

    for manifest_file in os.listdir(manifest_dir):
        manifest_path = os.path.join(manifest_dir, manifest_file)

        # generate old-style manifest hash for now
        # TODO: remove after conversion
        with open(manifest_path) as manifest_fp:
            data = json.load(manifest_fp)

        manifest_data = data["manifest"]

        # generate manifest id based on concatenated stage IDs calculated from osbuild
        manifest_id = get_manifest_id(manifest_data)
        id_fname = manifest_id + ".json"

        manifest_serialised = serialise(manifest_data)
        manifest_hash = hashlib.sha256(manifest_serialised.encode()).hexdigest()
        hash_fname = manifest_hash + ".json"

        build_request = data["build-request"]
        distro = build_request["distro"]
        arch = build_request["arch"]
        image_type = build_request["image-type"]

        # add manifest id to build request
        build_request["manifest-checksum"] = manifest_id

        if image_type in SKIPS:
            continue

        # check if the hash_fname exists in the synced directory
        dl_config_dir = os.path.join(dl_path, "builds", distro, arch)
        hash_config_path = os.path.join(dl_config_dir, hash_fname)
        id_config_path = os.path.join(dl_config_dir, id_fname)

        # TODO: remove after conversion
        if os.path.exists(hash_config_path):
            try:
                with open(hash_config_path) as dl_config_fp:
                    dl_config = json.load(dl_config_fp)
                commit = dl_config["commit"]
                print(f"Manifest {manifest_file} was successfully built in commit {commit}")
                # rename file in local s3 dir to new naming scheme
                os.rename(hash_config_path, id_config_path)
                continue
            except json.JSONDecodeError as jd:
                config_name = build_request["config"]["name"]
                errors.append((
                        f"failed to parse {hash_config_path}\n"
                        f"{jd.msg}\n"
                ))

        # check if the id_fname exists in the synced directory
        if os.path.exists(id_config_path):
            try:
                with open(id_config_path) as dl_config_fp:
                    dl_config = json.load(dl_config_fp)
                commit = dl_config["commit"]
                print(f"Manifest {manifest_file} was successfully built in commit {commit}")
                continue
            except json.JSONDecodeError as jd:
                config_name = build_request["config"]["name"]
                errors.append((
                        f"failed to parse {id_config_path}\n"
                        f"{jd.msg}\n"
                        "Scheduling config for rebuild\n"
                        f"Config: {distro}/{arch}/{image_type}/{config_name}\n"
                ))

        build_requests.append(build_request)

    print("Config filtering done!\n")
    if errors:
        # print errors at the end so they're visible
        print("Errors:")
        print("\n".join(errors))

    print("Syncing new files to s3 build cache")
    resync_s3_configs(dl_path)
    return build_requests


def list_images(arch_arg):
    out, err = runcmd(["go", "run", "./cmd/list-images", "-json", "-arches", arch_arg])
    return json.loads(out)


def u(s):
    return s.replace("-", "_")


def generate_configs(build_requests, pipeline_file):
    print(f"Generating dynamic pipelines for {len(build_requests)} builds")
    for build in build_requests:
        distro = build["distro"]
        arch = build["arch"]
        image_type = build["image-type"]
        config = build["config"]
        man_checksum = build["manifest-checksum"]

        config_name = config["name"]

        extra_cmds = []
        if image_type in OSTREE_CONTAINERS:
            # push to container registry and label with checksum
            build_name = f"{u(distro)}-{u(arch)}-{u(image_type)}-{u(config_name)}"
            # NOTE: the artifact path and filename depend on the names in the manifest
            container_path = f"./build/{build_name}/container/container.tar"
            extra_cmds.append(f'./tools/ci/push-container.sh "{container_path}" "{build_name}:build-{man_checksum}"')

        config_path = os.path.join(CONFIGS_PATH, config_name+".json")
        pipeline_file.write(JOB_TEMPLATE.format(distro=distro, arch=arch, image_type=image_type,
                                                config_name=config_name, config=config_path,
                                                extra_commands="\n".join(extra_cmds),
                                                internal="true" if "rhel" in distro else "false"))
    print("DONE!")


def main():
    config_path = sys.argv[1]
    arch_arg = sys.argv[2]

    check_config_names()

    manifest_dir = os.path.join(TEST_CACHE_ROOT, "manifests")
    generate_manifests(manifest_dir, arch_arg)
    build_requests = filter_builds(manifest_dir)

    with open(config_path, "w") as config_file:
        if len(build_requests) == 0:
            print("No manifest changes detected. Generating null config.")
            config_file.write(NULL_CONFIG)
            return

        config_file.write(BASE_CONFIG)
        generate_configs(build_requests, config_file)


if __name__ == "__main__":
    main()
